{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "19b9dffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Usuario\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.layers import LSTM, Dropout, Dense, Input\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.models import Model\n",
    "import string\n",
    "import re\n",
    "import tensorflow as tf \n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "import contractions\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589df68a",
   "metadata": {},
   "source": [
    "# In this notebook we show how to use GloVe embeddings to increase the performance of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "47b344f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('IMDBDataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bb1a35c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-18-bec8e15ffa9d>:6: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['clean_review'] = df['clean_review'].str.replace('[{}]'.format(string.punctuation), ' ')\n"
     ]
    }
   ],
   "source": [
    "# We make only two preprocess. Remove tags and punctuations\n",
    "\n",
    "def remove_tags(string):\n",
    "    result = re.sub('<.*?>','',string)\n",
    "    return result\n",
    "    \n",
    "df['clean_review']= df['review'].apply(lambda cw : remove_tags(cw))\n",
    "df['clean_review'] = df['clean_review'].str.replace('[{}]'.format(string.punctuation), ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9666a3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "review = df.review\n",
    "\n",
    "sentiment = df['sentiment']\n",
    "\n",
    "# Replcae \"positive\" with 1 and \"negative\" with 0\n",
    "y = np.array(list(map(lambda x: 1 if x==\"positive\" else 0, sentiment)))\n",
    "\n",
    "# Split into train and test\n",
    "X_train, X_test,Y_train, Y_test = train_test_split(review, y, test_size=0.2, random_state = 45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a5362e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text and create a word index so we can the merge it with the embedding\n",
    "\n",
    "tokenizer = Tokenizer(num_words=50000)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "words_to_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fb993e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to read the glove vector\n",
    "def read_glove_vector(glove_vec):\n",
    "    with open(glove_vec, 'r', encoding='UTF-8') as f:\n",
    "        words = set()\n",
    "        word_to_vec_map = {}\n",
    "        for line in f:\n",
    "            w_line = line.split()\n",
    "            curr_word = w_line[0]\n",
    "            word_to_vec_map[curr_word] = np.array(w_line[1:], dtype=np.float64)\n",
    "\n",
    "    return word_to_vec_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8186750f",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_vec_map = read_glove_vector('glove.6B.50d.txt')\n",
    "\n",
    "maxLen = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6c320101",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_len = len(words_to_index)\n",
    "embed_vector_len = word_to_vec_map['moon'].shape[0]\n",
    "\n",
    "# Create a mtrix where we would have each word embedding. And if there is no vector for a given word, keep it with 0's\n",
    "emb_matrix = np.zeros((vocab_len, embed_vector_len))\n",
    "\n",
    "not_found = []\n",
    "for word, index in words_to_index.items():\n",
    "    embedding_vector = word_to_vec_map.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        emb_matrix[index, :] = embedding_vector\n",
    "    else:\n",
    "        not_found.append(word)\n",
    "\n",
    "# Create the word embedding we are going to use in the model, replace the weights for those we create above\n",
    "embedding_layer = Embedding(input_dim=vocab_len, output_dim=embed_vector_len, input_length=maxLen, weights = [emb_matrix], trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "adf92433",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41650"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Words in the vocabulary that weren't found\n",
    "len(not_found)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fc2ed211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the LSTM model with embedding layer\n",
    "\n",
    "def imdb_rating(input_shape):\n",
    "\n",
    "    X_indices = Input(input_shape)\n",
    "\n",
    "    embeddings = embedding_layer(X_indices)\n",
    "\n",
    "    X = LSTM(128, return_sequences=True)(embeddings)\n",
    "\n",
    "    X = Dropout(0.6)(X)\n",
    "\n",
    "    X = LSTM(128, return_sequences=True)(X)\n",
    "\n",
    "    X = Dropout(0.6)(X)\n",
    "\n",
    "    X = LSTM(128)(X)\n",
    "\n",
    "    X = Dense(1, activation='sigmoid')(X)\n",
    "\n",
    "    model = Model(inputs=X_indices, outputs=X)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "51cbba62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the text into chunks of indexes already set\n",
    "X_train_indices = tokenizer.texts_to_sequences(X_train)\n",
    "\n",
    "# Pad sequences to 150\n",
    "X_train_indices = pad_sequences(X_train_indices, maxlen=maxLen, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "21c17ddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 150)]             0         \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 150, 50)           5612100   \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 150, 128)          91648     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 150, 128)          0         \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 150, 128)          131584    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 150, 128)          0         \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 128)               131584    \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,967,045\n",
      "Trainable params: 354,945\n",
      "Non-trainable params: 5,612,100\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = imdb_rating((maxLen,))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1e3c5bd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "625/625 [==============================] - 439s 696ms/step - loss: 0.5942 - accuracy: 0.6787\n",
      "Epoch 2/15\n",
      "625/625 [==============================] - 404s 646ms/step - loss: 0.5219 - accuracy: 0.7450\n",
      "Epoch 3/15\n",
      "625/625 [==============================] - 377s 603ms/step - loss: 0.4958 - accuracy: 0.7608\n",
      "Epoch 4/15\n",
      "625/625 [==============================] - 367s 587ms/step - loss: 0.4777 - accuracy: 0.7735\n",
      "Epoch 5/15\n",
      "625/625 [==============================] - 367s 587ms/step - loss: 0.4566 - accuracy: 0.7857\n",
      "Epoch 6/15\n",
      "625/625 [==============================] - 365s 584ms/step - loss: 0.4412 - accuracy: 0.7962\n",
      "Epoch 7/15\n",
      "625/625 [==============================] - 33596s 54s/step - loss: 0.4295 - accuracy: 0.8015\n",
      "Epoch 8/15\n",
      "625/625 [==============================] - 421s 674ms/step - loss: 0.4145 - accuracy: 0.8115\n",
      "Epoch 9/15\n",
      "625/625 [==============================] - 438s 700ms/step - loss: 0.3966 - accuracy: 0.8206\n",
      "Epoch 10/15\n",
      "625/625 [==============================] - 438s 701ms/step - loss: 0.3880 - accuracy: 0.8255\n",
      "Epoch 11/15\n",
      "625/625 [==============================] - 416s 665ms/step - loss: 0.3817 - accuracy: 0.8276\n",
      "Epoch 12/15\n",
      "625/625 [==============================] - 442s 707ms/step - loss: 0.3785 - accuracy: 0.8316\n",
      "Epoch 13/15\n",
      "625/625 [==============================] - 455s 728ms/step - loss: 0.3725 - accuracy: 0.8336\n",
      "Epoch 14/15\n",
      "625/625 [==============================] - 394s 631ms/step - loss: 0.3702 - accuracy: 0.8337\n",
      "Epoch 15/15\n",
      "625/625 [==============================] - 417s 667ms/step - loss: 0.3649 - accuracy: 0.8378\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22c8797aa60>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adam = tf.keras.optimizers.Adam(learning_rate = 0.0001)\n",
    "model.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train_indices, Y_train, batch_size=64, epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2da16f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_indices = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "X_test_indices = pad_sequences(X_test_indices, maxlen=maxLen, padding='post')\n",
    "\n",
    "model.evaluate(X_test_indices, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6236787",
   "metadata": {},
   "source": [
    "# Create the same process and model, but this time use the preprocess we use in the other notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "8500ab6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('IMDBDataset.csv')\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "wordnet_lemmatizer.lemmatize('reviewers')\n",
    "\n",
    "#defining the function for lemmatization\n",
    "def lemmatizer(text):\n",
    "    lemm_text = [wordnet_lemmatizer.lemmatize(word) for word in text.split()]\n",
    "    return ' '.join(lemm_text)\n",
    "\n",
    "a= lemmatizer(df['review'][0])\n",
    "\n",
    "def expand_contraction(text):\n",
    "    # creating an empty list\n",
    "    expanded_words = []    \n",
    "    for word in text.split():\n",
    "      # using contractions.fix to expand the shotened words\n",
    "      expanded_words.append(contractions.fix(word))   \n",
    "\n",
    "    return ' '.join(expanded_words)\n",
    "\n",
    "#Removing the html strips\n",
    "def strip_html(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "#Define function for removing special characters\n",
    "def remove_special_characters(text, remove_digits=True):\n",
    "    pattern=r'[^a-zA-z0-9\\s]'\n",
    "    text=re.sub(pattern,'',text)\n",
    "    return text\n",
    "\n",
    "def decontracted(phrase):\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase\n",
    "\n",
    "df['clean_review'] = df['review'].apply(expand_contraction)\n",
    "\n",
    "df['clean_review'] = df['review'].apply(decontracted)\n",
    "\n",
    "df['clean_review'] = df['clean_review'].apply(strip_html)\n",
    "\n",
    "df['clean_review'] = df['clean_review'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "e0986e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "review = df.clean_review\n",
    "\n",
    "sentiment = df['sentiment']\n",
    "\n",
    "y = np.array(list(map(lambda x: 1 if x==\"positive\" else 0, sentiment)))\n",
    "\n",
    "X_train, X_test,Y_train, Y_test = train_test_split(review, y, test_size=0.2, random_state = 45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "58f2319a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=50000)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "words_to_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "4e9c025c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_len = len(words_to_index)\n",
    "embed_vector_len = word_to_vec_map['moon'].shape[0]\n",
    "\n",
    "emb_matrix = np.zeros((vocab_len, embed_vector_len))\n",
    "\n",
    "not_found = []\n",
    "for word, index in words_to_index.items():\n",
    "    embedding_vector = word_to_vec_map.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        emb_matrix[index, :] = embedding_vector\n",
    "    else:\n",
    "        not_found.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "e7512c54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34405"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Doing this preprocess we have less words not found!\n",
    "\n",
    "len(not_found)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "2d2d593d",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(input_dim=vocab_len, output_dim=embed_vector_len, input_length=maxLen, weights = [emb_matrix], trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "0ca48cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_indices = tokenizer.texts_to_sequences(X_train)\n",
    "\n",
    "X_train_indices = pad_sequences(X_train_indices, maxlen=maxLen, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "1b6ce9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = imdb_rating((maxLen,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "fbfcc5da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "625/625 [==============================] - 524s 832ms/step - loss: 0.5882 - accuracy: 0.6776\n",
      "Epoch 2/15\n",
      "625/625 [==============================] - 598s 957ms/step - loss: 0.5082 - accuracy: 0.7541\n",
      "Epoch 3/15\n",
      "625/625 [==============================] - 583s 932ms/step - loss: 0.4809 - accuracy: 0.7714\n",
      "Epoch 4/15\n",
      "625/625 [==============================] - 584s 934ms/step - loss: 0.4656 - accuracy: 0.7789\n",
      "Epoch 5/15\n",
      "625/625 [==============================] - 3748s 6s/step - loss: 0.4402 - accuracy: 0.7939\n",
      "Epoch 6/15\n",
      "625/625 [==============================] - 588s 940ms/step - loss: 0.4274 - accuracy: 0.8049\n",
      "Epoch 7/15\n",
      "625/625 [==============================] - 595s 951ms/step - loss: 0.4137 - accuracy: 0.8110\n",
      "Epoch 8/15\n",
      "625/625 [==============================] - 591s 946ms/step - loss: 0.3995 - accuracy: 0.8194\n",
      "Epoch 9/15\n",
      "625/625 [==============================] - 13999s 22s/step - loss: 0.3899 - accuracy: 0.8270\n",
      "Epoch 10/15\n",
      "625/625 [==============================] - 613s 981ms/step - loss: 0.3805 - accuracy: 0.8281\n",
      "Epoch 11/15\n",
      "625/625 [==============================] - 606s 970ms/step - loss: 0.3812 - accuracy: 0.8293\n",
      "Epoch 12/15\n",
      "625/625 [==============================] - 611s 978ms/step - loss: 0.3723 - accuracy: 0.8351\n",
      "Epoch 13/15\n",
      "625/625 [==============================] - 609s 974ms/step - loss: 0.3674 - accuracy: 0.8379\n",
      "Epoch 14/15\n",
      "625/625 [==============================] - 614s 983ms/step - loss: 0.3646 - accuracy: 0.8375\n",
      "Epoch 15/15\n",
      "625/625 [==============================] - 595s 952ms/step - loss: 0.3573 - accuracy: 0.8422\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x17ba611aa60>"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adam = tf.keras.optimizers.Adam(learning_rate = 0.0001)\n",
    "model.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train_indices, Y_train, batch_size=64, epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "b8ff3d6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 39s 120ms/step - loss: 0.3736 - accuracy: 0.8327\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.37362515926361084, 0.8327000141143799]"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_indices = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "X_test_indices = pad_sequences(X_test_indices, maxlen=maxLen, padding='post')\n",
    "\n",
    "model.evaluate(X_test_indices, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7831f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save embedding so we can use it in tensor board projector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d46618c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "found = []\n",
    "word_found = []\n",
    "for word, index in words_to_index.items():\n",
    "    embedding_vector = word_to_vec_map.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        found.append(embedding_vector)\n",
    "        word_found.append(word)\n",
    "\n",
    "import io\n",
    "\n",
    "out_v = io.open('vecs.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('meta.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "k = 0\n",
    "for i in range(len(found)):\n",
    "    if k != 0:\n",
    "        out_m.write('\\n')\n",
    "        out_v.write('\\n')\n",
    "    out_m.write(word_found[i])\n",
    "    out_v.write('\\t'.join([str(x) for x in found[i]]))\n",
    "    k += 1\n",
    "out_v.close()\n",
    "out_m.close() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
